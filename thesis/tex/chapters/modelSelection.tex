%! Author = jacobcarlson
%! Date = 4/26/23

\section{Model Selection}

Different models were tested, under the same input, to test the potential of using jump distributions to classify
malicious code samples. Multiple models were used throughout the testing off all opcode sets and KL divergence
method combinations. The python SciKit Learn framework was used to implement models, since it offers quick
implementation and reduces risk of manual error. The Linear SVM model was used as a ground truth due to simplicity.
Both Logistic Regression and the Multi-Layer Perception, MLP, Network were more accurate than the Linear SVM model.
When Logistic Regression and MLP Network were compared pairwise, under the same stipulations, the MLP network was more
accurate $(p=1.0)$.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{l|S|S}
            \textbf{Model} & \textbf{Z} & \textbf{p}\\
            \hline
            Ridge Regression & -1.14 & 0.13\\
            Stochastic Gradient Descent & -46.13 & 0.00\\
            Logistic Regression & 13.34 & 1.00\\
            Multi-Layer Perceptron Network & 48.89 & 1.00\\
        \end{tabular}
        \label{tab:table1}
        \caption{Probability of Model being more accurate than Linear SVM}
        \footnotesize{Z scores and Probabilities of listed model being more accurate than a Scikit Learn’s Linear SVM
        when tested with a Wilcoxon Signed Rank test, using the differences between the accuracies of samples tested on
        the same data under the same conditions.}

    \end{center}
\end{table}

Scaling the data before inputting it into the model proved to result in a more accurate model, when each model was
compared its unscaled results. The scaled MLP network was also compared to all other scaled models, and no model was
determined to be more accurate.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{l|S|S}
            \textbf{Model} & \textbf{Z} & \textbf{p}\\
            \hline
            Linear SVM & 11.73 & 1.00\\
            Ridge Regression & 23.36 & 1.00\\
            Logistic Regression & 8.22 & 1.00\\
            Multi-Layer Perceptron Network & 9.44 & 1.00\\
        \end{tabular}
        \label{tab:table2}
        \caption{Probability of Scaled Models being more accurate than non-Scaled Models}
        \footnotesize{Z scores and Probabilities of model being more accurate when data is scaled using SciKit Learns
        Standard Scaler than when data is not scaled, tested with a Wilcoxon Signed Rank test using the differences
        between the accuracies of samples tested on the same data under the same conditions.}

    \end{center}
\end{table}

\begin{table}[H]
    \begin{center}
        \begin{tabular}{l|S|S}
            \textbf{Model} & \textbf{Z} & \textbf{p}\\
            \hline
            Linear SVM & -50.76 & 0.00\\
            Logistic Regression & -51.59 & 0.00\\
            Stochastic Gradient Descent & -55.51 & 0.00\\
            Logistic Regression & -49.21 & 0.00\\
        \end{tabular}
        \centering
        \label{tab:table3}
        \caption{Probability of Scaled Models being more accurate than Scaled Multi-Layer Perceptron Network}
        \footnotesize{Z scores and Probabilities of model being more accurate than a scaled Multi-Layer Perceptron
        Network with a Wilcoxon Signed Rank test, using the differences between the accuracies of samples tested on the
        same data under the same conditions.}

    \end{center}
\end{table}

The Scaled Multi-Layer Perceptron Network was the most accurate under all parameter sets. This is not a novel finding,
as it had a much higher level of learnable parameters than the model it was compared against. The MLP Network had three
hidden layers ($[100, 200, 50]$) and ran for 300 iterations, the rest of the parameters were unchanged from SciKit Learn’s
default parameters. These parameter changes were decided purely because they were round numbers and larger than the
original input. These attributes are most likely not optimal, and the model could be refined to increase the overall
accuracy of the process, but similar to testing the granularity of the distribution, refining accuracy through standard
means is not the intention of this work.