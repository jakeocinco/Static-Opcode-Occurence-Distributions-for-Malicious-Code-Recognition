%! Author = jacobcarlson
%! Date = 4/27/23

\section{Test Results and Conclusion}

The final model was tested against multiple datasets which were not used to train or verify any models.
A set of benign files was obtained from analyzing all files with a $.exe$ extension on a clean install of a Windows 10
virtual machine.

Three malicious file sets were downloaded from VirusShare\cite{virusShare1,virusShare5,virusShare451}, an online malware repository for security researchers.
Each file set had over $130,000$  malware samples, but only $15,000$ executables were used from each set due to
processing time constraints.
The first two datasets from VirusShare ($VirusShare\_00000$ and $VirusShare\_00005$) were written in 2012,
while $VirusShare\_00451$ was written more recently.
These malicious files are curated from user submissions, so there is reason to believe that most of the files in
$VirusShare\_00451$ are from current systems.

A final benign set was obtained from a macbook running macOS Big Sur, by analyzing all executable files
that had $.bundle$, $.dylib$, or $.so$ extensions.
Table \ref{tab:testTable} shows the average accuracy of each test set, which is tested on $40$ models each trained
against different combinations of ground truth and training samples with $250$ random selected samples from each test
used to determine the accuracy.



\begin{center}
    \underline{Final Model Parameters}\\
    Opcode Set - Malicious Only\\
    KL Divergence - $log_{10}(D_{KL}(dist||x))$\\
    Bins - 100\\
    Model - Multi-Layer Perceptron, \\
    \footnotesize{Scaled Input}
    \footnotesize{hidden layers - [100, 200, 50]}
\end{center}


\begin{table}[H]
    \begin{center}
        \captionsetup{justification=centering}
        \caption{Average Accuracy of models on purely Test Datasets}
%        \footnotesize{Test accuracies and standard deviations are obtained using ten different samples ground truth
%        sample pairs, tested 4 times each using a different random seed each time to differ the data.
%        Resulting in 40 different test cases.}
%        \footnotesize{\\$Opcode\; Set: Malicious Only$\\$KL\; Divergence\; Method: log_{10}(D_{KL}(dist||x))$}
        \begin{tabular}{l|l|r}
            \textbf{Sample Set} & \textbf{Class} & \textbf{Accuracy}\\
            \hline
            Windows10 Virtual & Benign & $79.3\% \pm 5.8\%$\\
            VirusShare\_00000 & Malicious & $98.4\% \pm 0.9\%$\\
            VirusShare\_00005 & Malicious & $97.5\% \pm 1.3\%$\\
            VirusShare\_00451 & Malicious & $94.7\% \pm 3.5\%$\\
            MacOS Big Sur & Benign & $35.8\% \pm 4.4\%$\\
        \end{tabular}
        \label{tab:testTable}
    \end{center}
\end{table}

The malicious data sets were more accurate than the accuracies from validation sets during training,
while the benign samples faltered below that accuracy.
The weighted average of the accuracies is almost identical to that of the tests conducted on these models during training
when ignoring the MacOS files.
There was no reason to believe that the MacOS files would be accurately classified when using a model trained on
executable obtained from a Windowâ€™s 7 operating system\cite{lester}, but the fact that it had an accuracy
less than a random decision maker would suggest that there is data to be learned from the MacOS
samples, it just does not correlate with the windows data.

The increase in accuracy over the Malicious data sets could be due to the difference in samples under the
training set and test sets.
The training set classifies malicious files based of whether multiple antivirus software identifies it as
malicious\cite{lester}, while VirusShare is cultivated by a digital forensics team\cite{virusShare1,virusShare5,virusShare451}.
This more hands on approach could mean that the malicious samples in the test set are more malicious and differ more
from benign files than the malicious samples in training did.

The more recent VirusShare sample was less accurate than the older VirusShare samples.
The models were trained on Windows 7 files\cite{lester}, which was the most popular windows operating system at the
time the older VirusShare sets were curated\cite{statcounter}.
The models most likely performed very strongly on malicious code from Windows 7 Operating Systems since that what
it was trained on, it is even possible that there is overlap between samples but there is no way to verify that.
Similar logic could be used to attribute the low accuracy of the Windows 10 samples.
